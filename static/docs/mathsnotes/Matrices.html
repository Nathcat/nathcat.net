<!DOCTYPE html>
<html>
    <head>
        <title>Matrices (Lecture 15)</title>

        <script type="text/javascript"
            src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js">
        </script>
    </head>

    <body>
        <h1>Matrices</h1>
        <h2>Matrix-Matrix multiplication</h2>
        <p>
            Given two matrices 
            $\displaystyle{A:\mathbb{R}^n\to\mathbb{R}^m}$
            and 
            $\displaystyle{B:\mathbb{R}^m\to\mathbb{R}^k}$,
            we have:
        </p>
        <p>
            $\displaystyle{
                BA:\mathbb{R}^n\to\mathbb{R}^k
            }$
        </p>
        <p>
            $\displaystyle{
                \vec{x}\mapsto(BA)\vec{x}=B(A\vec{x})
            }$
        </p>
        <p>
            To multiply matrices, the number of columns in the first matrix
            must match the number of rows in the second matrix:
        </p>
        <p>
            $\displaystyle{
                \mathcal{M}(k,m)\times\mathcal{M}(m,n)\to\mathcal{M}(k,n)
            }$
        </p>
        <p>
            With the coefficients of the result matrix $C=BA$ being given by: 
            $\displaystyle{
                C_{ij}=\sum_{h=1}^mb_{ih}a_{hj}
            }$
        </p>
        <p>
            This operation is associative: $C(BA)=(CB)A$, but not commutative,
            so it <i><b>might</b></i> not be true that $AB=BA$, it might not even
            be possible to define both products.
        </p>
        <p>
            And multiplying with an identity matrix ($I_n\in\mathcal{M}(n,n)$):
        </p>
        <p>
            Let $A\in\mathcal{M}(m,n)$, $I_mA=A$ and $AI_n=A$
        </p>

        <h2>Rotation matrices</h2>
        <p>
            Matrices are very often used to define linear transformations, for
            example rotations on vectors:
        </p>
        <p>
            Rotation around $x$ : $\displaystyle{
                \left(
                \begin{array}{ccc}
                1&0&0\\
                0&\cos(\alpha)&\sin(\alpha)\\
                0&-\sin(\alpha)&\cos(\alpha)
                \end{array}    
                \right)
            }$
        </p>
        <p>
            Rotation around $y$ : $\displaystyle{
                \left(
                \begin{array}{ccc}
                \cos(\alpha)&0&\sin(\alpha)\\
                0&1&0\\
                -\sin(\alpha)&0&\cos(\alpha)
                \end{array}    
                \right)
            }$
        </p>
        <p>
            Rotation around $z$ : $\displaystyle{
                \left(
                \begin{array}{ccc}
                \cos(\alpha)&\sin(\alpha)&0\\
                -\sin(\alpha)&\cos(\alpha)&0\\
                0&0&1
                \end{array}    
                \right)
            }$
        </p>

        <h2>Matrix exponentiation</h2>
        <p>
            For $A=\mathcal{M}(n,n)$ we can define a notion of exponetiation on $A$.
        </p>
        <p>
            $\displaystyle{
                A^k=\{
                    \begin{array}{cc}
                    I_n&\mathrm{if } k=0\\
                    AA^{k-1}=A^{k-1}A&\mathrm{if } k>0
                    \end{array}
            }$
        </p>

        <h2>Matrix inverses</h2>
        <p>
            For $A,B\in\mathcal{M}(n,n)$, if $AB=BA=I_n$, we say that $B$ is
            the inverse of $A$, and write $B=A^{-1}$.
        </p>
        <p>
            For example: $\displaystyle{
                \left(
                \begin{array}{cc}
                2&-4\\ 0&2
                \end{array}    
                \right)

                \left(
                \begin{array}{cc}
                0.5&1\\ 0&0.5
                \end{array}    
                \right)
                =
                \left(
                \begin{array}{cc}
                2\cdot 0.5&2-4\cdot 0.5\\ 0&2\cdot 0.5
                \end{array}    
                \right)
                =
                \left(
                \begin{array}{cc}
                1&0\\0&1
                \end{array}    
                \right)
            }$
        </p>
        <p>
            Although sometimes the inverse of a matrix may simply not exist.
        </p>
        
        <h3>Row operations</h3>
        <p>
            We can try to build $A^{-1}$ by seeing how to transform $A$ into
            $I_n$ by "row reduction".
        </p>
        <p>
            However, reading the slides on this is confusing me so maybe we just
            stick to dividing by the determinant of the matrix:
        </p>
        <p>
            $A^{-1}=\frac{A}{det(A)}$
        </p>

        <h2>Determinant of a Matrix</h2>
        <p>
            The determinant can be defined as a recursive function with the
            following base case:
        </p>
        <p>
            $\displaystyle{
                A=\left[
                \begin{array}{cc}
                a&b\\c&d
                \end{array}
                \right]\to
                det\left(A\right)=ad-bc
            }$
        </p>
        <p>
            Look at a 3x3 matrix:
        </p>
        <p>
            $\displaystyle{
                A=\left[
                \begin{array}{ccc}
                a&b&c\\d&e&f\\g&h&i
                \end{array}
                \right]\to
                det\left(A\right)=a\left(ei-fh\right)-b\left(di-fg\right)+c\left(dh-eg\right)
            }$
        </p>
        <p>
            This is described nicely in the following links:
        </p>
        <ul>
            <li><a href="https://www.mathsisfun.com/algebra/matrix-determinant.html">Math Is Fun</a></li>
            <li><a href="https://en.wikipedia.org/wiki/Laplace_expansion">Laplace Expansion on Wikipedia</a></li>
        </ul>

        <p>
            The determinant only exists on square matrices.
        </p>

        <h2>Final notes</h2>
        <p>An Orthogonal matrix is one which satisfies $A^T=A^{-1}$</p>
    </body>
</html>